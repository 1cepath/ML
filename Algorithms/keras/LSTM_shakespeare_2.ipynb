{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM shakespeare 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/getmubarak/ML/blob/master/Algorithms/keras/LSTM_shakespeare_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSnBEnte6Lx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "afdd5ef8-f425-44d9-f7e9-1e2e9f23eef4"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "path_to_file"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/shakespeare.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaPxUU5rfX22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHAKESPEARE_TXT = '/root/.keras/datasets/shakespeare.txt'\n",
        "\n",
        "def transform(txt):\n",
        "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "\n",
        "def input_fn(seq_len=1, batch_size=1024):\n",
        "  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
        "  with tf.io.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
        "    txt = f.read()\n",
        "\n",
        "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)\n",
        "\n",
        "  def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "  BUFFER_SIZE = 10000\n",
        "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  return ds.repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAui9zFRfg7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build the model\n",
        "\n",
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "  source = tf.keras.Input(\n",
        "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  #Because our vocabulary size is 256, the input dimension to the Embedding layer is 256. \n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
        "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFv0NDnGfkgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "training_model = lstm_model(seq_len=1, stateful=False)\n",
        "training_model.compile(\n",
        "      optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGFSAYsQg6Nj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd3eea79-914c-4655-ad42-9a921ac6aa57"
      },
      "source": [
        "\n",
        "\n",
        "training_model.fit(\n",
        "    input_fn(),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=1\n",
        ")\n",
        "training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 61s 612ms/step - loss: 2.4499 - sparse_categorical_accuracy: 0.0797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsmWRNlOiUsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "f826bb32-8efd-4c20-e700-d9fef9a8682c"
      },
      "source": [
        "#Make predictions with the model\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "\n",
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "  prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "  \n",
        "  # sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  generated = ''.join([chr(c) for c in p])  # Convert back to text\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " br le g llld'so, vitheley; scee hadintheikeed io br s?\n",
            "I teery fo cree tor p, tse, g ret.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "INENGoubl thilol sod,\n",
            "\n",
            "Har gr n be hee my h ithe\n",
            "t wes at heean ges th; de I br lor tol f or ade wis wat and yom, hice cowonome zil,\n",
            "iair mont asoncueve se\n",
            "\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " deso th kist fre asssh co we ntol bistag t d ue thy of, tyo ie lo hide d m t leey th he, m, minge we hiwimove cord aw t ams h mibrarnt t ame yout lody tie stheand g itly abeves made earinerd hamsotonope os ked oe imaitiren, th he 'sthay se pun urite\n",
            "\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            " umyoathaure spagely treal dowol,'seae hesord w we wn is meespl thopefer. wis d yoounere tilse l ide igs thov h gey atow rf l ne Mu ntey, bey to ds onrit anste 's odle, twin kilodif we walabre ald lar, t y's wan brpre toto k m.\n",
            "A is, s be ur chow.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " w t It t hibe tcee d ma'do we bamee. faf t pealikit brion-ve reatathe mor ugidiror a m'tomo,\n",
            "\n",
            "Whean sised thesshel, meak gen toreo oor n 'tieis ts som t at, m my, lid, osse forfe r'airoth! te ben is Anor tingit t theam\n",
            " of y m, irced n gshe at we is\n",
            "\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " m ico lade' t feefigonodowo st focou wsel y lee clt bl tes's ad s nee y ith thaneecere sh weqrer is f thot, ts t th yo isorsond giks tit nink d ave he,\n",
            "Wis s t ame, te k ar te 'd t wo t meaaf momeimares, gueear sspar tss se Aun of an d m ndee f my c\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBqbl7c8fm3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}