{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0. 0.]\n",
      "recall: [0. 0.]\n",
      "f1score: [0. 0.]\n",
      "support: [3 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "actual = [1,1,1,0,0,0]\n",
    "pred =[0,0,0,1,1,1]\n",
    "\n",
    "precision, recall, fscore, support = score(actual, pred)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('f1score: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion Matrix\n",
    "\n",
    "           predicated        predicted \n",
    "            positives        negatives\n",
    "\n",
    "positives    tp                 fn  \n",
    "\n",
    "negatives    fp                 tn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision = tp÷(tp+fp)\n",
    "\n",
    "High precision = no garbage (all dog prediction are really dogs)\n",
    "\n",
    "We have got 0.788 precision which is pretty good.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recall = tp ÷ (tp+fn)\n",
    "\n",
    "High Recall = (all dogs are predicated as dogs)\n",
    "\n",
    "We have got recall of 0.631 which is good for this model as it’s above 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "High recall, low precision\n",
    "all dogs are predicated as dogs, but also classifies cats also as dogs\n",
    "\n",
    "Our classifier thinks a lot of things are “hot dogs”; legs on beaches, fries and whatnot. However it also thinks a lot \n",
    "of “hot dogs” are “hot dogs”. So from our set of images we got a lot of images classified as “hot dogs”, many of them \n",
    "was in the set of actual “hot dogs”, however a lot of them were also “not hot dogs”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Low recall, high precision\n",
    "all dog prediction are really dogs, but misses a lot of dogs\n",
    "\n",
    "Our classifier is very picky, and does not think many things are hot dogs. All the images it thinks are “hot dogs”, \n",
    "are really “hot dogs”. However it also misses a lot of actual “hot dogs”, because it is so very picky. We have low \n",
    "recall, but a very high precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "High recall, high precision\n",
    "\n",
    "Our classifier is very good, it is very picky, but still it gets almost all of the images of “hot dogs” which are “hot \n",
    "dogs” correct. We are happy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1\n",
    "-------\n",
    "It is also called the F Score or the F Measure.\n",
    "F1 score conveys the balance between the precision and the recall.\n",
    "\n",
    "f1 score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support\n",
    "--------------\n",
    "The support is the number of occurrences of each class in y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
